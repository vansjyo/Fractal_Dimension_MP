# -*- coding: utf-8 -*-
"""GPUTrial_Nolds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T4dUxiNzOYxiteQy22rpaURLr8it0alF

## > Import libraries
"""

## Install and Import packages
# !pip install nolds
# import nolds
import numpy as np
import pandas as pd
from scipy.io import loadmat
from tabulate import tabulate
from sklearn import preprocessing
from matplotlib import pyplot as plt
from scipy.signal import savgol_filter

"""## > Mount Drive"""

## Mounting Google Drive2
#from google.colab import drive
#drive.mount('/content/drive')

import multiprocessing as mp
import math
from numba import jit, njit, vectorize, cuda
print("Number of processors: ", mp.cpu_count())

#!find / -iname 'libdevice'
#!find / -iname 'libnvvm.so'

#import os
#os.environ['NUMBAPRO_LIBDEVICE'] = "/usr/local/cuda-10.0/nvvm/libdevice"
#os.environ['NUMBAPRO_NVVM'] = "/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so"

"""## > Import Data"""

## Import data
img = loadmat('Indian_pines_corrected.mat')
img_gt = loadmat('Indian_pines_gt.mat')
img = img['indian_pines_corrected']
img_gt = img_gt['indian_pines_gt']
height, width, bands = img.shape[0], img.shape[1], img.shape[2]

#reshape data to 2-D array and normalize the reflectance values
img = np.reshape(img, [height*width, bands])
img_gt = np.reshape(img_gt, [height*width,])
img = preprocessing.normalize(img.astype('float32'))
num_classes = len(np.unique(img_gt))

print("Number of Classes: ", num_classes)

#store indices of each class in a list of lists (ignoring the background class label 0)
class_dist = [] 
for i in range(1, num_classes):
    #fetch indices corresponding to class i 
    class_index = np.nonzero(img_gt == i)[0].tolist()
    #append the obtained list as an item to the main list 
    class_dist.append(class_index)

"""## > VarshaPendyala
> https://github.com/varshapendyala/Manifold-Learning
"""

import time
def intrinsic_est(lookup,original_data):	
	C = correlation_dim(original_data)
	print(c)
	intrinsic_dim = lookup(C)
	print(intrinsic_dim)
	return intrinsic_dim 


def Distance(x):
	MinDist = 1e20
	MaxDist = 0
	Radius = np.zeros(32)
	for i in range(x.shape[0]-1,0,-1):
		Distances = DistVectPoint(x[0:i,:],x[i,:])
		#print i
		DistanceNoZero = ElimZero(Distances, 1e-10)
		minval = min(DistanceNoZero)
		maxval = max(Distances)
		if MinDist > minval :
			MinDist = minval
		if MaxDist < maxval :
			MaxDist = maxval
	for k in range(32):
		Radius[k] = np.exp(np.log(MinDist)+(k+1)*(np.log(MaxDist)-np.log(MinDist))/32)
	return Radius
	
def ElimZero(Distances,Tolerance):
	SigDist = Distances-Tolerance
	SigDist = ((np.sign(np.sign(SigDist*-1)-0.5))+1)*1e20
	DistanceNoZero = Distances + SigDist
	return DistanceNoZero

def BinFilling(x,Radius):
	NoPoints = x.shape[0]
	BinCount = np.zeros(32)
	for i in range(x.shape[0]-1,0,-1):
		Distances = DistVectPoint(x[0:i,:],x[i,:])
		for j in range(32):
			BinCount[j] = BinCount[j] + CountPoints(Distances,Radius[j])
	BinCount = BinCount/((NoPoints)*(NoPoints-1)/2)
	return BinCount

def DistVectPoint(data,point):
	Diffe = np.zeros((data.shape[0],data.shape[1]))
	for i in range(data.shape[1]):
		Diffe[:,i] = data[:,i] - point[i]
	Diffe = Diffe**2
	Distances = np.sum(Diffe,1)
	Distances = np.sqrt(Distances)
	return Distances

def CountPoints(Distances, Threshold):
	NumofPoints = np.size(Distances)
	ThresholdMatr = np.ones(NumofPoints)*Threshold
	CountVect = np.sum(Distances<ThresholdMatr)
	return CountVect

def Slope(Radius,BinCount,centre,high):
	lnRadius = np.log(Radius)
	lnBinCount = np.log(BinCount)
	Max = 0
	Min = lnBinCount[0]
	IntervalHigh = (Max-Min)*high 
	Top = -((Max-Min)*(1-centre)) + (IntervalHigh/2)
	Base = -((Max-Min)*(1-centre)) - (IntervalHigh/2)
	RelDataX = []
	RelDataY = []
 
	for i in range(32):
		if ((lnBinCount[i] >= Base) and (lnBinCount[i]<=Top)):
			RelDataX.append(lnRadius[i])
			RelDataY.append(lnBinCount[i])
			
	RelDataX = np.array(RelDataX)
	RelDataY = np.array(RelDataY)
	P = np.polyfit(RelDataX,RelDataY,1)
	Slope = P[0]
	return Slope

def correlation_dim(d_data):
	x = d_data
	start_time = time.time() 
	Radius = Distance(x)
	print("--- %s seconds ---" % (time.time() - start_time))
	start_time = time.time()
	BinCount = BinFilling(x,Radius)
	print("--- %s seconds ---" % (time.time() - start_time))
	start_time = time.time() 
	RadiusNormal = Radius/Radius[31]
	print("--- %s seconds ---" % (time.time() - start_time))
	plt.loglog(RadiusNormal,BinCount,basex=np.e,basey=np.e)
	Slp = Slope(Radius,BinCount,0.6,0.125)
	plt.show()	
	return Slp

x = img[img_gt!=0]
shufflePermutation = np.random.permutation(len(x))
x = x[shufflePermutation]
x = x[:2000,:]
h = correlation_dim(x)
print(h)

"""## > Notsebastiano
> https://github.com/notsebastiano/GP_algorithm/blob/master/GP.py
"""

import matplotlib.pyplot as plt
import numpy as np
from scipy import stats

x = img[img_gt!=0]
x = x[:2000,]
def td_embedding(timeseries,emb,tau):
  indexes = np.arange(0,emb,1)*tau
  return np.array([timeseries[indexes +i] for i in range(len(timeseries)-(emb-1)*tau)])


def logarithmic_r(min_n, max_n, factor):
  if max_n <= min_n:
    raise ValueError("arg1 has to be < arg2")
  if factor <= 1:
    raise ValueError("factor(arg3) has to be > 1")
  max_i = int(np.floor(np.log(1.0 * max_n / min_n) / np.log(factor)))
  return np.array([min_n * (factor ** i) for i in range(max_i + 1)])


def grassberg_procaccia(timeseries,emb_dim,time_delay,plot = None):
  '''
  Implementation of the Gassberger-Procaccia algorithm to estimate the
  correlation dimension of a set of points in an m-dimensional space.
  This code takes in input a timeseries of scalar values and the embedding dimension + time delay
  necessary to perform a time-delay embedding in phase space to reconstruct the attractor
  Args:
    timeseries: array of scalars
    emb_dim: (int) embedding dimension
    time_delay = (int) time delay between values in phase space reconstruction
  Kwargs:
    plot: if set to True: plots the logarithm of the correlation
    sums against the logarithm of the set of values of r considered in the algorithm
  r is the scaling factor, it tells the threshold distance between points. if we have a plateau
  of local slopes means that we are in a scaling range.
  Returns:
    Correlation dimension (scalar)
  '''
  sd_data = np.std(timeseries)
  orbit = td_embedding(timeseries, emb_dim, time_delay)

  n = len(orbit)
  r_vals = logarithmic_r(0.1 * sd_data, 0.7 * sd_data, 1.03)
  distances = np.zeros(shape=(n,n))
  r_matrix_base = np.zeros(shape=(n,n))

  for i in range(n):
    for j in range(i,n):
      distances[i][j] = np.linalg.norm(orbit[i]-orbit[j])
      r_matrix_base[i][j] = 1

  C_r = []
  for r in r_vals:
    r_matrix = r_matrix_base*r
    heavi_matrix = np.heaviside( r_matrix - distances, 0)
    corr_sum = (2/float(n*(n-1)))*np.sum(heavi_matrix)
    C_r.append(corr_sum)

  #strong assumption: the log-log plot is assumed to be a smooth, monotonic function,
  #hence the slope in the scaling region should be the maximum gradient ( in this case
  #is taken as the mean of the last five maximum gradients as they are calculated for every point )

  gradients = np.gradient(np.log2(C_r),np.log2(r_vals))
  gradients.sort()
  D = np.mean(gradients[-5:])

  if plot:
    plt.plot(np.log2(r_vals),np.log2(C_r))
    plt.show()
  
  return D

"""## > Main - Test"""

threadsperblock = 16
blocks = (200 + (threadsperblock - 1)) // threadsperblock
blocks_per_grid = (xblocks)

@vectorize(['int64(int64, int64)'], target='cuda')
def add_ufunc_gpu(x, y):
    return x + y

#pick first pixel
x = 15 
print(x)
x = img[img_gt!=0,:]
shufflePermutation = np.random.permutation(len(x))
x = x[shufflePermutation]
x = x[:1000,:]

#should be equal to number of bands
print("X shape: ", x.shape) 
#Initialize info storing table
table = [] 
#array for the sequential FD plot - FD vs #attributes considered
FD_plot = []
#Initialize diff storing 2-D array - initialize difference storage var to 1 as to act as +inf for change in FD
diff = np.arange(bands*bands*1.0).reshape((bands, bands))*0 + 1 
#store matrix- x[i,j] denote the fractal dimension in the i'th iteration after removing the j'th band in x(updated one)
x1_matrix = np.arange(bands*bands*1.0).reshape((bands, bands))*0 + 1 
#setting optimal dimension as 5. We will inspect the plot obtained and then come here again to change the value.
opt_dim = 5

def tryme(i, j):
    #find FD
    h = correlation_dim(x)
    #deleting a band
    x1 = np.delete(x, j, 1)
    # #sanity check
    # print("X shape: ", x.shape, " X1 shape: ", x1.shape)
    #find partial FD
    h1 = correlation_dim(x1)
    #find absolute difference between FD and partial FD 
    global diff
    diff[i,j] = abs(h1-h) 
    #store the fractal dimension after removal of that band 
    global x1_matrix
    x1_matrix[i,j] = h1

## Main - Find optimal Dimension - Supervised

import multiprocessing as mp

#Iterating after removal of one band with min Fractal change
for i in range(bands-opt_dim):
    #Iterating to find band with min Fractal change
    pool = mp.Pool(mp.cpu_count())

    pool.starmap(tryme, [(i, iter) for iter in range(bands-i)])

    pool.close()

    #compute index of min difference in FD and partial FD  
    min_index_col = np.argmin(diff[i,], axis=0) 
    #Store details in info table
    table.append([ i, diff[i,min_index_col], min_index_col, x.shape])
    #store FD values of min difference column
    FD_plot.append(x1_matrix[i, min_index_col])
    #sanity checks
    print(x.shape)
    print(min_index_col)
    #reset x after deleting band causing min change in FD or having highest correlation
    x = np.delete(x, min_index_col, 1)

## Main - Find optimal Dimension - Supervised

#pick first pixel
x = img[img_gt!=0,:]
shufflePermutation = np.random.permutation(len(x))
x = x[shufflePermutation]
x = x[:1000,:]
# plt.plot(x)
#should be equal to number of bands
print("X shape: ", x.shape) 

#Initialize info storing table
table = [] 
#array for the sequential FD plot - FD vs #attributes considered
FD_plot = []
#Initialize diff storing 2-D array - initialize difference storage var to 1 as to act as +inf for change in FD
diff = np.arange(bands*bands*1.0).reshape((bands, bands))*0 + 1 
#store matrix- x[i,j] denote the fractal dimension in the i'th iteration after removing the j'th band in x(updated one)
x1_matrix = np.arange(bands*bands*1.0).reshape((bands, bands))*0 + 1 
#setting optimal dimension as 5. We will inspect the plot obtained and then come here again to change the value.
opt_dim = 5
#Iterating after removal of one band with min Fractal change
b = cuda.grid(1)

for i in range(bands-opt_dim):
    #Iterating to find band with min Fractal change
    for j in range(bands-i):
        #find FD
        h = correlation_dim(x)
        #deleting a band
        x1 = np.delete(x, j, 1)
        #sanity check
        print("X shape: ", x.shape, " X1 shape: ", x1.shape)
        #find partial FD
        h1 = correlation_dim(x1)
        #find absolute difference between FD and partial FD 
        diff[i,j] = abs(h1-h) 
        #store the fractal dimension after removal of that band 
        x1_matrix[i,j] = h1

    #compute index of min difference in FD and partial FD  
    min_index_col = np.argmin(diff[i,], axis=0) 
    #Store details in info table
    table.append([ i, diff[i,min_index_col], min_index_col, x.shape])
    #store FD values of min difference column
    FD_plot.append(x1_matrix[i, min_index_col])
    #sanity checks
    print(x.shape)
    print(min_index_col)
    #reset x after deleting band causing min change in FD or having highest correlation
    x = np.delete(x, min_index_col, 1)

"""## > Results

### > Table
"""

#defining table headers
headers = ["Iteration", "Minimum fractal Diff", "Band with Min Diff", "New Shape"]
#print table
print(tabulate(table, headers, tablefmt="github"))
#save the info table in a CSV file
df = pd.DataFrame(table, columns= headers)
df.to_csv (r'/content/drive/My Drive/Major_Project/Test_Results/FD_iter.csv', index = False, header=True)

"""### > Plot"""

#make an array of the min differences (change in FD) 
min_fd = np.min(diff, axis=1)

#plot the differnce as function of removed bands
fig = plt.figure()
plt.plot(min_fd[:193])
fig.suptitle('Fractal Dimension Difference vs Bands Removed')
plt.xlabel('ith iteration (i bands removed)')
plt.ylabel('Difference in FD')
#save the result
fig.savefig('/content/drive/My Drive/Major_Project/Test_Results/Result1.jpg',dpi=300)

"""### > Smooth"""

#smooth the fractal plot over a zoomed window frame
y = savgol_filter(min_fd[170:193], 9,3)
#display the smoothed plot and save the results
fig1 = plt.figure()
plt.plot(y)
fig1.savefig('/content/drive/My Drive/Major_Project/Test_Results/Result2.jpg',dpi=300)